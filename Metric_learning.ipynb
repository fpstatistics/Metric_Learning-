{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myouter(x):\n",
    "    return np.outer(x,x)\n",
    "def gradient(y,X,A,i):\n",
    "    y_i_hat = N_W_estimator(y,X,A,i)  \n",
    "    X_j = np.delete(X - X[i,:],i,axis=0)\n",
    "    X_T_X = np.apply_along_axis(myouter,1,X_j)\n",
    "    A_T_A = np.dot(A.T,A)\n",
    "    X_A_T_A_X = np.diag(np.dot(np.dot(X_j,A_T_A),X_j.T))\n",
    "    K =  norm.pdf( np.sqrt(2 * X_A_T_A_X ) )\n",
    "    dy = y_i_hat - np.delete(y,i)\n",
    "    other = np.apply_along_axis(np.multiply,0,X_T_X,K * dy)\n",
    "    partial_A = 4 * np.dot(A, (y_i_hat - y[i]) * np.sum(other,0))\n",
    "    return partial_A\n",
    "\n",
    "def N_W_estimator(y, X, A, i):\n",
    "    X_j = np.delete(X - X[i,:],i,axis=0)\n",
    "    A_T_A = np.dot(A.T,A)\n",
    "    X_A_T_A_X = np.diag(np.dot(np.dot(X_j,A_T_A),X_j.T))\n",
    "\n",
    "    K =  norm.pdf( np.sqrt(2 * X_A_T_A_X ) )\n",
    "    molecular = np.sum( K * np.delete(y,i) )\n",
    "    denominator = np.sum(K)\n",
    "    y_i_hat = molecular/ denominator\n",
    "    return y_i_hat\n",
    "# ##计算梯度时忽略掉那些很小的k_ij\n",
    "#     if y_i_hat <1e13:\n",
    "    for j in np.delete(range(n), i):\n",
    "        X_ij = X[i,:] - X[j,:]\n",
    "        A_X_ij = np.dot(A, X_ij)\n",
    "        k_ij = norm.pdf( np.sqrt(2*sum(A_X_ij**2)) )\n",
    "        sum_k_ij = sum_k_ij + k_ij\n",
    "        other = other + k_ij * np.einsum('i,j->ij', X_ij, X_ij) * (y_i_hat - y[j])\n",
    "    partial_A = 4 * np.dot(A , (y_i_hat - y[i] )  * other)\n",
    "#     else:\n",
    "#         partial_A = 0\n",
    "        \n",
    "    return partial_A\n",
    "\n",
    "\n",
    "def Loss(y,X,A):\n",
    "    n = np.shape(X)[0]\n",
    "    y_hat = [N_W_estimator(y,X,A,i) for i in range(n)]\n",
    "#     y_hat = np.array(y_hat)\n",
    "    sh1 = [i for i in range(n) if y_hat[i]<1e13 ] ##舍去那些估计量为零的样本点\n",
    "    y_hat = np.array(y_hat)\n",
    "    loss = np.sum((y[sh1] - y_hat[sh1])**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_LR(data_x, data_y, d, maxepochs=500,epsilon=1e-4):\n",
    "    n, p = np.shape(data_x)\n",
    "    A_ = np.eye(max(p,d))\n",
    "    if p >= d:\n",
    "        A = A_[0:d,:]\n",
    "    else:\n",
    "        A = A_[:,0:p]\n",
    "#     A = np.array([[1,0],[0,1],[1,1]]).T\n",
    "#     A = np.array([[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1]]).T\n",
    "    alpha_0 = 0.05\n",
    "    epochs_count = 0\n",
    "    loss_list = []\n",
    "    epochs_list = []\n",
    "    starttime = datetime.datetime.now()\n",
    "    while (epochs_count < maxepochs ):\n",
    "        rand_i = np.random.randint(n)  # 随机取一个样本\n",
    "        loss = Loss(data_y,data_x,A)\n",
    "#         print(loss)\n",
    "        grad = gradient(data_y,data_x,A,rand_i) #损失函数的梯度\n",
    "        A_temp = A - alpha_0 * grad\n",
    "        loss_new_temp = Loss(data_y,data_x,A_temp)\n",
    "        if( loss_new_temp - loss < 0 ): #要求迭代之后的损失函数必须下降，否则就重新随机一个样本在进行迭代\n",
    "            A = A_temp\n",
    "            loss_new = loss_new_temp\n",
    "        else:\n",
    "            loss_new = 1e10  \n",
    "        if abs(loss_new - loss) < epsilon:\n",
    "            break\n",
    "        epochs_list.append(epochs_count)\n",
    "        epochs_count += 1\n",
    "    loss = loss_new_temp\n",
    "    endtime = datetime.datetime.now()  \n",
    "    time_consump = endtime - starttime\n",
    "    time_consump = time_consump.seconds\n",
    "    return [A,loss,time_consump,epochs_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_dimension(K,X,Y):\n",
    "    n,p = X.shape\n",
    "    kf = KFold(n_splits = K , shuffle = False, random_state = 12)\n",
    "    A_list = []\n",
    "    mean_error_list = []\n",
    "    param_grid = [2,3]\n",
    "    i = 0\n",
    "    for param in param_grid:\n",
    "        error_list = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index,:] , X[test_index,:]\n",
    "            Y_train, Y_test = Y[train_index] , Y[test_index] \n",
    "            re = SGD_LR(X_train, Y_train,d = param, maxepochs = 50,epsilon=1e-3)\n",
    "            A = re[0]\n",
    "            A_A_T = np.dot(A,A.T)\n",
    "            A_A_T_inv = np.linalg.inv(A_A_T)\n",
    "            A_P = np.dot( np.dot(A.T,A_A_T_inv), A)\n",
    "            ###test_error\n",
    "      \n",
    "            y_est = [N_W_estimator(Y_test, X_test, A, i) for i in range(len(Y_test))]\n",
    "            err = sum((Y_test - y_est)**2)\n",
    "            error_list.append(err)\n",
    "        mean_error_list.append(np.mean(error_list))\n",
    "        i+=1\n",
    "        print(str(i) + 'parameter has been searched')\n",
    "    indx = mean_error_list.index(min(mean_error_list))\n",
    "    dimension_hat = param_grid[indx]\n",
    "            \n",
    "    return [dimension_hat,mean_error_list]\n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
